clf<-fit(clf, Tomorrow_bin ~ Today, data = regression_df)
clf$fit$y
View(clf)
clf<-decision_tree(mode="regression")
clf<-fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$y<-clf$fit$y>0.5
clf$fit$y
s
#we can calculate the accuarcy using score
score = sum(regression_df$Tomorrow_bin==clf$fit$y) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$y, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
clf<- fit(clf, Tomorrow ~ Today, data = regression_df)
# Fit the random forest model
clf <- rand_forest(mode="regression")
clf<- fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$predictions
clf$fit$y<-clf$fit$y>0.5
clf$fit$predictions<-clf$fit$predictions>0.5
library(parsnip)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df[seq(1,20,1),])
# Fit the random forest model
clf <- rand_forest(mode="regression")
clf<- fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$predictions<-clf$fit$predictions>0.5
#we can calculate the accuarcy using score
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
score = sum(regression_df$Tomorrow_bin==clf$fit$predictions) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$y, regression_df$Tomorrow_bin)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$predictions, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
library(parsnip)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df[seq(1,20,1),])
# Train decision tree using parsnip, a great library for training models!
# Create binary variable for rain tomorrow
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
clf<-decision_tree(mode="regression")
clf<-fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$y<-clf$fit$y>0.5
#we can calculate the accuarcy using score
score = sum(regression_df$Tomorrow_bin==clf$fit$y) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$y, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
View(clf)
# Here is an example of how to build and populate
# a heuristic model
library(tidyverse)
df <- read.csv("~/data/seattle_weather_1948-2017.csv")
numrow = 25549
heuristic_df <- data.frame("Yesterday" = 0,
"Today" = 0,
"Tomorrow" = 0,
"Guess" = FALSE,
"Rain Tomorrow" = FALSE,
"Correct" = FALSE,
"True Positive" = FALSE,
"False Positive" = FALSE,
"True Negative" = FALSE,
"False Negative" = FALSE)
df$PRCP = ifelse(is.na(df$PRCP),
ave(df$PRCP, FUN = function(x) mean(x, na.rm = TRUE)),
df$PRCP)
for (z in 1:numrow){
i = z + 2
yesterday = df[i-2,2]
today = df[i-1,2]
tomorrow = df[i,2]
if (tomorrow == 0){
rain_tomorrow = FALSE
}else{
rain_tomorrow = TRUE
}
heuristic_df[z,1] = yesterday
heuristic_df[z,2] = today
heuristic_df[z,3] = tomorrow
heuristic_df[z,4] = FALSE # Label all guesses as false
heuristic_df[z,5] = rain_tomorrow
heuristic_df[z,7] = FALSE
heuristic_df[z,8] = FALSE
heuristic_df[z,9] = FALSE
heuristic_df[z,10] = FALSE
if ((today > 0) & (yesterday > 0)){
heuristic_df[z,4] = TRUE
}
if (heuristic_df[z,4] == heuristic_df[z,5]){
heuristic_df[z,6] = TRUE
if (heuristic_df[z,4] == TRUE){
heuristic_df[z,7] = TRUE #true positive
}else{
heuristic_df[z,9] = TRUE #True negative
}
}else{
heuristic_df[z,6] = FALSE
if (heuristic_df[z,4] == TRUE){
heuristic_df[z,7] = TRUE #false positive
}else{
heuristic_df[z,9] = TRUE #false negative
}
}
}
View(heuristic_df)
df <- read_csv('~/data/seattle_weather_1948-2017.csv')
df[is.na(df)]<-0
# Create an empty dataframe
heuristic_df<-data.frame(matrix(, nrow=nrow(df)-2, ncol=0))
heuristic_df$Yesterday<-0
heuristic_df$Today<-0
heuristic_df$Tomorrow<-0
heuristic_df$Guess<-FALSE
heuristic_df$Rain_tomorrow<-FALSE
heuristic_df$Correct<-FALSE
# View first 10 rows of each dataframe
View(df[seq(1,10,1),])
head(heuristic_df)
head(heuristic_df, 10)
# View first 10 rows of each dataframe
head(df, 10)
head(heuristic_df, 10)
for(z in seq(1,nrow(df)-2,1)) {
#start at time 2 in the data frame
i <- z + 2
#pull values from the dataframe
yesterday <- df[(i-2),2]
today <- df[(i-1),2]
tomorrow <- df[i,2]
rain_tomorrow <- tomorrow>0
heuristic_df[z,1] <- yesterday
heuristic_df[z,2] <- today
heuristic_df[z,3] <- tomorrow
heuristic_df[z,4] <- FALSE # set guess default to False
heuristic_df[z,5] <- rain_tomorrow
######### uncomment and create your heuristic guess ################
#if( ##### your conditions here #########){
#    heuristic_df[z,4] <- TRUE
# }
####################################################################
if(heuristic_df[z,4] == heuristic_df[z,5])  heuristic_df[z,6] <- TRUE
else heuristic_df[z,6] <- FALSE
}
sum(heuristic_df$Correct)/nrow(heuristic_df)
# Logistic Regression
# Using the same seattle weather data as last chapter develop a logistic regression model
#import the pakcages that we will need
library(ggplot2)
library(glmnet)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
# View first 20 rows
head(regression_df,20)
# Train Logistic Model
# Create binary variable for rain tomorrow
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
clf <- glm(Tomorrow_bin ~ Today, data = regression_df, family = "binomial")
#we can calculate the accuarcy using the score method
clf$fitted.values<-clf$fitted.values>=0.5
score = sum(regression_df$Tomorrow_bin==clf$fitted.values) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fitted.values, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
#import the pakcages that we will need
library(ggplot2)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Intercept<-1
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,3] <- tomorrow
regression_df[i,2] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
#this makes a simple dataframe with a relationship that we can now plot
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow))
# Creating a basic linear model to best predict these values.
# Start with a slope and intercept values of 1 and then iterate through gradient descent.
gradientDescent <- function(X, y, param, alpha, num_iters){
for (i in 1:num_iters){
y_hat = as.matrix(X)%*%param
param = param - alpha*(t(as.matrix(X))%*%(as.matrix(y_hat-y)))
}
return(param)
}
# In this fucntion param is the initial guess of the values of the linear function,
# X is the vector of data values and y is the realization
X <- regression_df[0:200,1:2]
y <- as.data.frame(regression_df[0:200,3])
param <- matrix(c(1,1),nrow = 2)
alpha <- 0.0001
num_iters <- 1000
solution <- gradientDescent(X, y, param, alpha, num_iters)
solution
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow))+
geom_abline(intercept=solution[1], slope=solution[2])
mymodel <- lm(regression_df$Today ~ regression_df$Tomorrow, data = regression_df)
print(mymodel)
regression_df$Predictions<-predict.lm(mymodel)
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow), color="black")+
geom_point(mapping = aes(x = Today, y = Predictions), color="blue")+
geom_abline(intercept=solution[1], slope=mymodel$coefficients[2], color = "blue")
# using the r2 (pronounced r squared) value we can get a basic measure of model quality
summary(mymodel)$r.squared
ggplot(regression_df) +
geom_point(mapping = aes(y = Predictions, x = Tomorrow), color="black")+
scale_y_continuous(limits = c(0,5))+
scale_x_continuous(limits = c(0,5))
library(parsnip)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df[seq(1,20,1),])
head(regression_df,20)
# Fit the random forest model
clf <- rand_forest(mode="regression")
clf<- fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$predictions<-clf$fit$predictions>0.5
#we can calculate the accuarcy using score
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
score = sum(regression_df$Tomorrow_bin==clf$fit$predictions) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$predictions, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
#import the pakcages that we will need
library(ggplot2)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Intercept<-1
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,3] <- tomorrow
regression_df[i,2] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df)
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
#this makes a simple dataframe with a relationship that we can now plot
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow))
# Creating a basic linear model to best predict these values.
# Start with a slope and intercept values of 1 and then iterate through gradient descent.
gradientDescent <- function(X, y, param, alpha, num_iters){
for (i in 1:num_iters){
y_hat = as.matrix(X)%*%param
param = param - alpha*(t(as.matrix(X))%*%(as.matrix(y_hat-y)))
}
return(param)
}
# Creating a basic linear model to best predict these values.
# Start with a slope and intercept values of 1 and then iterate through gradient descent.
gradientDescent <- function(X, y, param, alpha, num_iters){
for (i in 1:num_iters){
y_hat = as.matrix(X)%*%param
param = param - alpha*(t(as.matrix(X))%*%(as.matrix(y_hat-y)))
}
return(param)
}
# In this fucntion param is the initial guess of the values of the linear function,
# X is the vector of data values and y is the realization
X <- regression_df[0:200,1:2]
y <- as.data.frame(regression_df[0:200,3])
param <- matrix(c(1,1),nrow = 2)
alpha <- 0.0001
num_iters <- 1000
solution <- gradientDescent(X, y, param, alpha, num_iters)
solution
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow))+
geom_abline(intercept=solution[1], slope=solution[2])
mymodel <- lm(regression_df$Today ~ regression_df$Tomorrow, data = regression_df)
print(mymodel)
regression_df$Predictions<-predict.lm(mymodel)
View(regression_df)
# Look at the plots of real values in black and predicted values in blue
ggplot(regression_df) +
geom_point(mapping = aes(x = Today, y = Tomorrow), color="black")+
geom_point(mapping = aes(x = Today, y = Predictions), color="blue")+
geom_abline(intercept=solution[1], slope=mymodel$coefficients[2], color = "blue")
# using the r2 (pronounced r squared) value we can get a basic measure of model quality
summary(mymodel)$r.squared
ggplot(regression_df) +
geom_point(mapping = aes(y = Predictions, x = Tomorrow), color="black")+
scale_y_continuous(limits = c(0,5))+
scale_x_continuous(limits = c(0,5))
###################### adjust this code to add columns here #######################################
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,3] <- tomorrow
regression_df[i,2] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df)
# Logistic Regression
# Using the same seattle weather data as last chapter develop a logistic regression model
#import the pakcages that we will need
library(ggplot2)
library(glmnet)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
# View first 20 rows
head(regression_df,20)
# Train Logistic Model
# Create binary variable for rain tomorrow
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
clf <- glm(Tomorrow_bin ~ Today, data = regression_df, family = "binomial")
#we can calculate the accuarcy using the score method
clf$fitted.values<-clf$fitted.values>=0.5
score = sum(regression_df$Tomorrow_bin==clf$fitted.values) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fitted.values, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
numrows = 25547
###################### adjust this code to add columns here #######################################
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df)
library(parsnip)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
View(regression_df[seq(1,20,1),])
# Train decision tree using parsnip, a great library for training models!
# Create binary variable for rain tomorrow
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
View(regression_df)
clf<-decision_tree(mode="regression")
clf<-fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$y<-clf$fit$y>0.5
#we can calculate the accuarcy using score
score = sum(regression_df$Tomorrow_bin==clf$fit$y) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$y, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
library(parsnip)
df <- read.csv('~/data/seattle_weather_1948-2017.csv')
numrows <- 25549 # can be as large as 25549
#create an empty dataframe to hold values
regression_df<-data.frame(matrix(, nrow=numrows, ncol=0))
regression_df$Today<-0
regression_df$Tomorrow<-0
# Populate regression DF
for(i in seq(1,numrows,1)){
tomorrow <- df[(i+1),2]
today <-df[(i),2]
regression_df[i,2] <- tomorrow
regression_df[i,1] <- today
}
regression_df <- regression_df[complete.cases(regression_df),] #exclude any rows with missing data
head(regression_df,20)
# Fit the random forest model
clf <- rand_forest(mode="regression")
clf<- fit(clf, Tomorrow ~ Today, data = regression_df)
clf$fit$predictions<-clf$fit$predictions>0.5
#we can calculate the accuarcy using score
regression_df$Tomorrow_bin<-regression_df$Tomorrow>0
score = sum(regression_df$Tomorrow_bin==clf$fit$predictions) / length(regression_df$Tomorrow_bin)
print(score)
#we can also make a simple confusion matrix
confusion_matrix<-table(clf$fit$predictions, regression_df$Tomorrow_bin)
names(attributes(confusion_matrix)$dimnames)<-c("Predicted", "Actual")
confusion_matrix
#Here is a bit nicer matrix
image(confusion_matrix, xlab="Predicted", ylab="Actual", xaxt="n", yaxt="n")
axis(side = 1, at=c(0,1), labels=c("FALSE","TRUE"))
axis(side = 2, at=c(0,1), labels=c("FALSE","TRUE"))
